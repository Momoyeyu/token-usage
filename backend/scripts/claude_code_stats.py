#!/usr/bin/env python3
"""
Claude Code ä½¿ç”¨ç»Ÿè®¡è„šæœ¬
ç»Ÿè®¡ Claude Code åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ä½¿ç”¨æƒ…å†µ

æ•°æ®æ¥æºï¼š
1. ~/.claude/stats-cache.json - å…¨å±€ç»Ÿè®¡ç¼“å­˜
2. ~/.claude/projects/*/sessions-index.json - é¡¹ç›®ä¼šè¯ç´¢å¼•
3. ~/.claude/projects/*/*.jsonl - ä¼šè¯è¯¦ç»†æ¶ˆæ¯ï¼ˆåŒ…å«ç²¾ç¡® token æ•°æ®ï¼‰
"""

import json
import os
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path
from collections import defaultdict
import argparse


def get_claude_dir() -> Path:
    """è·å– Claude Code æ•°æ®ç›®å½•"""
    return Path.home() / ".claude"


def parse_timestamp(ts: str) -> datetime:
    """è§£æ ISO æ ¼å¼æ—¶é—´æˆ³ï¼Œè¿”å› UTC æ—¶é—´ï¼ˆå¸¦æ—¶åŒºä¿¡æ¯ï¼‰"""
    # å¤„ç†ä¸åŒæ ¼å¼çš„æ—¶é—´æˆ³
    if ts.endswith("Z"):
        ts = ts[:-1] + "+00:00"
    try:
        dt = datetime.fromisoformat(ts)
        # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡å®šä¸º UTC
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except ValueError:
        # å¤„ç†æ¯«ç§’
        dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt


def load_stats_cache() -> dict:
    """åŠ è½½å…¨å±€ç»Ÿè®¡ç¼“å­˜"""
    stats_file = get_claude_dir() / "stats-cache.json"
    if stats_file.exists():
        with open(stats_file, "r") as f:
            return json.load(f)
    return {}


def get_project_dirs() -> list[Path]:
    """è·å–æ‰€æœ‰é¡¹ç›®ç›®å½•"""
    projects_dir = get_claude_dir() / "projects"
    if not projects_dir.exists():
        return []
    return [d for d in projects_dir.iterdir() if d.is_dir()]


def load_sessions_index(project_dir: Path) -> dict:
    """åŠ è½½é¡¹ç›®çš„ä¼šè¯ç´¢å¼•"""
    index_file = project_dir / "sessions-index.json"
    if index_file.exists():
        with open(index_file, "r") as f:
            return json.load(f)
    return {"entries": []}


def analyze_jsonl_file(jsonl_path: Path, start_date: datetime, end_date: datetime) -> dict:
    """åˆ†æå•ä¸ª JSONL æ–‡ä»¶ï¼Œæå– token ä½¿ç”¨æ•°æ®"""
    stats = {
        "input_tokens": 0,
        "output_tokens": 0,
        "cache_creation_input_tokens": 0,
        "cache_read_input_tokens": 0,
        "user_messages": 0,
        "assistant_messages": 0,
        "tool_calls": 0,
        "models_used": defaultdict(lambda: {
            "input_tokens": 0,
            "output_tokens": 0,
            "cache_creation_input_tokens": 0,
            "cache_read_input_tokens": 0,
            "requests": 0,
        }),
        "by_day": defaultdict(lambda: {
            "input_tokens": 0,
            "output_tokens": 0,
            "cache_creation_input_tokens": 0,
            "cache_read_input_tokens": 0,
            "requests": 0,
        }),
    }

    if not jsonl_path.exists():
        return stats

    seen_request_ids = set()  # é¿å…é‡å¤è®¡ç®—åŒä¸€è¯·æ±‚çš„ token

    try:
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    record = json.loads(line)
                except json.JSONDecodeError:
                    continue

                # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨èŒƒå›´å†…
                timestamp_str = record.get("timestamp")
                if not timestamp_str:
                    continue

                try:
                    timestamp = parse_timestamp(timestamp_str)
                except (ValueError, TypeError):
                    continue

                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¥æœŸèŒƒå›´å†…
                if timestamp < start_date or timestamp > end_date:
                    continue

                record_type = record.get("type")

                if record_type == "user":
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼ˆéå·¥å…·ç»“æœï¼‰
                    message = record.get("message", {})
                    content = message.get("content", "")
                    if isinstance(content, str) or (
                        isinstance(content, list) and
                        any(c.get("type") == "text" for c in content if isinstance(c, dict))
                    ):
                        stats["user_messages"] += 1

                elif record_type == "assistant":
                    message = record.get("message", {})
                    usage = message.get("usage", {})
                    request_id = record.get("requestId", "")
                    model = message.get("model", "unknown")

                    # åªç»Ÿè®¡æ¯ä¸ªè¯·æ±‚ä¸€æ¬¡ï¼ˆé¿å…æµå¼å“åº”é‡å¤è®¡ç®—ï¼‰
                    if request_id and request_id in seen_request_ids:
                        continue
                    if request_id:
                        seen_request_ids.add(request_id)

                    if usage:
                        input_tokens = usage.get("input_tokens", 0)
                        output_tokens = usage.get("output_tokens", 0)
                        cache_creation = usage.get("cache_creation_input_tokens", 0)
                        cache_read = usage.get("cache_read_input_tokens", 0)

                        stats["input_tokens"] += input_tokens
                        stats["output_tokens"] += output_tokens
                        stats["cache_creation_input_tokens"] += cache_creation
                        stats["cache_read_input_tokens"] += cache_read

                        # æŒ‰æ¨¡å‹ç»Ÿè®¡
                        stats["models_used"][model]["input_tokens"] += input_tokens
                        stats["models_used"][model]["output_tokens"] += output_tokens
                        stats["models_used"][model]["cache_creation_input_tokens"] += cache_creation
                        stats["models_used"][model]["cache_read_input_tokens"] += cache_read
                        stats["models_used"][model]["requests"] += 1

                        # æŒ‰å¤©ç»Ÿè®¡
                        day = timestamp.date().isoformat()
                        stats["by_day"][day]["input_tokens"] += input_tokens
                        stats["by_day"][day]["output_tokens"] += output_tokens
                        stats["by_day"][day]["cache_creation_input_tokens"] += cache_creation
                        stats["by_day"][day]["cache_read_input_tokens"] += cache_read
                        stats["by_day"][day]["requests"] += 1

                    # ç»Ÿè®¡åŠ©æ‰‹æ¶ˆæ¯å’Œå·¥å…·è°ƒç”¨
                    content = message.get("content", [])
                    if isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict):
                                if item.get("type") == "text":
                                    stats["assistant_messages"] += 1
                                elif item.get("type") == "tool_use":
                                    stats["tool_calls"] += 1

    except Exception as e:
        print(f"Warning: Error reading {jsonl_path}: {e}", file=sys.stderr)

    return stats


def collect_stats(start_date: datetime, end_date: datetime, username: str = None) -> dict:
    """æ”¶é›†æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ç»Ÿè®¡æ•°æ®"""
    result = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "username": username or os.environ.get("USER", "unknown"),
            "machine": os.uname().nodename,
        },
        "summary": {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_cache_creation_tokens": 0,
            "total_cache_read_tokens": 0,
            "total_tokens": 0,  # input + output (ä¸å«ç¼“å­˜ï¼Œè¿™æ˜¯å®é™… API è°ƒç”¨çš„ token)
            "total_sessions": 0,
            "total_user_messages": 0,
            "total_assistant_messages": 0,
            "total_tool_calls": 0,
            "active_days": set(),
            "active_projects": set(),
        },
        "by_model": defaultdict(lambda: {
            "input_tokens": 0,
            "output_tokens": 0,
            "cache_creation_input_tokens": 0,
            "cache_read_input_tokens": 0,
            "requests": 0,
        }),
        "by_project": {},
        "by_day": defaultdict(lambda: {
            "input_tokens": 0,
            "output_tokens": 0,
            "cache_creation_input_tokens": 0,
            "cache_read_input_tokens": 0,
            "total_tokens_with_cache": 0,
        }),
    }

    project_dirs = get_project_dirs()

    for project_dir in project_dirs:
        project_name = project_dir.name
        sessions_index = load_sessions_index(project_dir)
        project_stats = {
            "input_tokens": 0,
            "output_tokens": 0,
            "cache_creation_input_tokens": 0,
            "cache_read_input_tokens": 0,
            "sessions": 0,
            "user_messages": 0,
            "assistant_messages": 0,
            "tool_calls": 0,
        }

        for entry in sessions_index.get("entries", []):
            session_id = entry.get("sessionId")
            if not session_id:
                continue

            # æ£€æŸ¥ä¼šè¯æ—¶é—´æ˜¯å¦å¯èƒ½åœ¨èŒƒå›´å†…
            created = entry.get("created")
            modified = entry.get("modified")
            if created:
                try:
                    created_dt = parse_timestamp(created)
                    # å¦‚æœä¼šè¯åˆ›å»ºæ—¶é—´æ™šäºç»“æŸæ—¥æœŸï¼Œè·³è¿‡
                    if created_dt > end_date:
                        continue
                except (ValueError, TypeError):
                    pass

            if modified:
                try:
                    modified_dt = parse_timestamp(modified)
                    # å¦‚æœä¼šè¯æœ€åä¿®æ”¹æ—¶é—´æ—©äºå¼€å§‹æ—¥æœŸï¼Œè·³è¿‡
                    if modified_dt < start_date:
                        continue
                except (ValueError, TypeError):
                    pass

            # åˆ†æ JSONL æ–‡ä»¶
            jsonl_path = project_dir / f"{session_id}.jsonl"
            session_stats = analyze_jsonl_file(jsonl_path, start_date, end_date)

            if session_stats["input_tokens"] > 0 or session_stats["user_messages"] > 0:
                project_stats["sessions"] += 1
                result["summary"]["total_sessions"] += 1
                result["summary"]["active_projects"].add(project_name)

                # è®°å½•æ´»è·ƒæ—¥æœŸ
                if created:
                    try:
                        result["summary"]["active_days"].add(parse_timestamp(created).date())
                    except (ValueError, TypeError):
                        pass

            # ç´¯åŠ ç»Ÿè®¡
            for key in ["input_tokens", "output_tokens", "cache_creation_input_tokens",
                       "cache_read_input_tokens", "user_messages", "assistant_messages", "tool_calls"]:
                project_stats[key] += session_stats[key]

            # æŒ‰æ¨¡å‹ç»Ÿè®¡
            for model, model_stats in session_stats["models_used"].items():
                for key in ["input_tokens", "output_tokens", "cache_creation_input_tokens",
                           "cache_read_input_tokens", "requests"]:
                    result["by_model"][model][key] += model_stats[key]

            # æŒ‰å¤©ç»Ÿè®¡
            for day, day_stats in session_stats["by_day"].items():
                result["by_day"][day]["input_tokens"] += day_stats["input_tokens"]
                result["by_day"][day]["output_tokens"] += day_stats["output_tokens"]
                result["by_day"][day]["cache_creation_input_tokens"] += day_stats["cache_creation_input_tokens"]
                result["by_day"][day]["cache_read_input_tokens"] += day_stats["cache_read_input_tokens"]
                # Calculate total with cache for this day
                day_total = (day_stats["input_tokens"] + day_stats["output_tokens"] +
                            day_stats["cache_creation_input_tokens"] + day_stats["cache_read_input_tokens"])
                result["by_day"][day]["total_tokens_with_cache"] += day_total
                # Track active days from actual data
                result["summary"]["active_days"].add(datetime.fromisoformat(day).date())

        # ç´¯åŠ åˆ°æ€»è®¡
        result["summary"]["total_input_tokens"] += project_stats["input_tokens"]
        result["summary"]["total_output_tokens"] += project_stats["output_tokens"]
        result["summary"]["total_cache_creation_tokens"] += project_stats["cache_creation_input_tokens"]
        result["summary"]["total_cache_read_tokens"] += project_stats["cache_read_input_tokens"]
        result["summary"]["total_user_messages"] += project_stats["user_messages"]
        result["summary"]["total_assistant_messages"] += project_stats["assistant_messages"]
        result["summary"]["total_tool_calls"] += project_stats["tool_calls"]

        if project_stats["sessions"] > 0:
            result["by_project"][project_name] = project_stats

    # è®¡ç®—æ€» token
    # API Token: input + outputï¼ˆå®é™… API è°ƒç”¨æ¶ˆè€—ï¼Œä¸å«ç¼“å­˜ï¼‰
    result["summary"]["total_tokens"] = (
        result["summary"]["total_input_tokens"] +
        result["summary"]["total_output_tokens"]
    )
    # å…¨é‡ Token: input + output + cache_read + cache_creationï¼ˆä¸ Cursor çš„ Total Tokens å¯¹åº”ï¼‰
    result["summary"]["total_tokens_with_cache"] = (
        result["summary"]["total_input_tokens"] +
        result["summary"]["total_output_tokens"] +
        result["summary"]["total_cache_read_tokens"] +
        result["summary"]["total_cache_creation_tokens"]
    )

    # è½¬æ¢ set ä¸º list
    result["summary"]["active_days"] = len(result["summary"]["active_days"])
    result["summary"]["active_projects"] = len(result["summary"]["active_projects"])

    # è½¬æ¢ defaultdict ä¸ºæ™®é€š dict
    result["by_model"] = dict(result["by_model"])
    result["by_day"] = dict(result["by_day"])

    return result


def format_tokens(n: int) -> str:
    """æ ¼å¼åŒ– token æ•°é‡"""
    if n >= 1_000_000:
        return f"{n/1_000_000:.2f}M"
    elif n >= 1_000:
        return f"{n/1_000:.1f}K"
    return str(n)


def print_summary(stats: dict):
    """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
    summary = stats["summary"]
    metadata = stats["metadata"]

    print("=" * 60)
    print("Claude Code ä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 60)
    print(f"\nç”¨æˆ·: {metadata['username']} @ {metadata['machine']}")
    print(f"ç»Ÿè®¡å‘¨æœŸ: {metadata['start_date'][:10]} ~ {metadata['end_date'][:10]}")
    print(f"ç”Ÿæˆæ—¶é—´: {metadata['generated_at'][:19]}")

    print("\nğŸ“Š Token ä½¿ç”¨é‡")
    print("-" * 40)
    print(f"  è¾“å…¥ Token:        {format_tokens(summary['total_input_tokens']):>12}")
    print(f"  è¾“å‡º Token:        {format_tokens(summary['total_output_tokens']):>12}")
    print(f"  ç¼“å­˜åˆ›å»º Token:    {format_tokens(summary['total_cache_creation_tokens']):>12}")
    print(f"  ç¼“å­˜è¯»å– Token:    {format_tokens(summary['total_cache_read_tokens']):>12}")
    print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"  API Token:         {format_tokens(summary['total_tokens']):>12} (input + output)")
    print(f"  å…¨é‡ Token:        {format_tokens(summary['total_tokens_with_cache']):>12} (å«ç¼“å­˜ï¼Œå¯æ¯” Cursor)")

    print("\nğŸ“ˆ æ´»åŠ¨ç»Ÿè®¡")
    print("-" * 40)
    print(f"  ä¼šè¯æ•°:            {summary['total_sessions']:>12}")
    print(f"  ç”¨æˆ·æ¶ˆæ¯æ•°:        {summary['total_user_messages']:>12}")
    print(f"  åŠ©æ‰‹æ¶ˆæ¯æ•°:        {summary['total_assistant_messages']:>12}")
    print(f"  å·¥å…·è°ƒç”¨æ•°:        {summary['total_tool_calls']:>12}")
    print(f"  æ´»è·ƒå¤©æ•°:          {summary['active_days']:>12}")
    print(f"  æ´»è·ƒé¡¹ç›®æ•°:        {summary['active_projects']:>12}")

    if stats["by_model"]:
        print("\nğŸ¤– æŒ‰æ¨¡å‹ç»Ÿè®¡")
        print("-" * 40)
        for model, model_stats in stats["by_model"].items():
            model_total = model_stats["input_tokens"] + model_stats["output_tokens"]
            print(f"  {model}:")
            print(f"    è¯·æ±‚æ•°: {model_stats['requests']}, Token: {format_tokens(model_total)}")

    if stats["by_project"]:
        print("\nğŸ“ æŒ‰é¡¹ç›®ç»Ÿè®¡ (Top 5)")
        print("-" * 40)
        sorted_projects = sorted(
            stats["by_project"].items(),
            key=lambda x: x[1]["input_tokens"] + x[1]["output_tokens"],
            reverse=True
        )[:5]
        for project_name, project_stats in sorted_projects:
            # ç®€åŒ–é¡¹ç›®åæ˜¾ç¤º
            display_name = project_name.replace("-Users-admin-Projects-", "").rstrip("-")
            project_total = project_stats["input_tokens"] + project_stats["output_tokens"]
            print(f"  {display_name}:")
            print(f"    ä¼šè¯: {project_stats['sessions']}, Token: {format_tokens(project_total)}")

    print("\n" + "=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="ç»Ÿè®¡ Claude Code ä½¿ç”¨æƒ…å†µ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s                    # ç»Ÿè®¡æœ¬å‘¨ï¼ˆå‘¨ä¸€åˆ°ä»Šå¤©ï¼‰
  %(prog)s --days 7           # ç»Ÿè®¡æœ€è¿‘ 7 å¤©
  %(prog)s --start 2026-01-24 --end 2026-01-30  # æŒ‡å®šæ—¥æœŸèŒƒå›´
  %(prog)s --output stats.json # è¾“å‡ºåˆ° JSON æ–‡ä»¶
        """
    )
    parser.add_argument("--start", type=str, help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end", type=str, help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--days", type=int, help="ç»Ÿè®¡æœ€è¿‘ N å¤©")
    parser.add_argument("--week", action="store_true", help="ç»Ÿè®¡æœ¬å‘¨ï¼ˆå‘¨ä¸€åˆ°ä»Šå¤©ï¼‰")
    parser.add_argument("--username", type=str, help="ç”¨æˆ·åï¼ˆç”¨äºå›¢é˜Ÿæ±‡æ€»ï¼‰")
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--json", action="store_true", help="ä»…è¾“å‡º JSONï¼ˆä¸æ‰“å°æ‘˜è¦ï¼‰")

    args = parser.parse_args()

    # ç¡®å®šæ—¥æœŸèŒƒå›´ï¼ˆä½¿ç”¨ UTC æ—¶åŒºä»¥åŒ¹é…æ—¥å¿—ä¸­çš„æ—¶é—´æˆ³ï¼‰
    now = datetime.now(timezone.utc)
    today = now.replace(hour=23, minute=59, second=59, microsecond=999999)

    if args.start and args.end:
        start_date = datetime.strptime(args.start, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        end_date = datetime.strptime(args.end, "%Y-%m-%d").replace(
            hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc
        )
    elif args.days:
        end_date = today
        start_date = (now - timedelta(days=args.days - 1)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )
    else:
        # é»˜è®¤ï¼šæœ¬å‘¨ï¼ˆå‘¨ä¸€åˆ°ä»Šå¤©ï¼‰
        end_date = today
        # è®¡ç®—æœ¬å‘¨ä¸€
        days_since_monday = now.weekday()
        start_date = (now - timedelta(days=days_since_monday)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )

    # æ”¶é›†ç»Ÿè®¡
    stats = collect_stats(start_date, end_date, args.username)

    # è¾“å‡º
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        if not args.json:
            print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {args.output}")

    if args.json:
        print(json.dumps(stats, ensure_ascii=False, indent=2))
    elif not args.output:
        print_summary(stats)
    else:
        print_summary(stats)


if __name__ == "__main__":
    main()
