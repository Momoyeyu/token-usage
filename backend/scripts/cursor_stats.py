#!/usr/bin/env python3
"""
Cursor ä½¿ç”¨ç»Ÿè®¡è„šæœ¬
è§£æ Cursor å¯¼å‡ºçš„ CSV æ–‡ä»¶ï¼Œç»Ÿè®¡ä½¿ç”¨æƒ…å†µ

CSV åˆ—è¯´æ˜ï¼š
- Date: æ—¶é—´æˆ³ (ISO æ ¼å¼)
- User: ç”¨æˆ·é‚®ç®±
- Kind: è¯·æ±‚ç±»å‹ (On-Demand, Errored, No Charge)
- Model: æ¨¡å‹åç§°
- Max Mode: æ˜¯å¦ä½¿ç”¨ Max æ¨¡å¼
- Input (w/ Cache Write): è¾“å…¥ tokenï¼ˆåŒ…å«ç¼“å­˜å†™å…¥ï¼‰
- Input (w/o Cache Write): è¾“å…¥ tokenï¼ˆä¸å«ç¼“å­˜å†™å…¥ï¼‰
- Cache Read: ç¼“å­˜è¯»å– token
- Output Tokens: è¾“å‡º token
- Total Tokens: æ€» token
- Requests: è¯·æ±‚æ•°ï¼ˆå¯èƒ½æ˜¯å°æ•°ï¼Œè¡¨ç¤ºåŠ æƒï¼‰
"""

import csv
import json
import os
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path
from collections import defaultdict
import argparse
import glob


def parse_timestamp(ts: str) -> datetime:
    """è§£æ ISO æ ¼å¼æ—¶é—´æˆ³"""
    if ts.endswith("Z"):
        ts = ts[:-1] + "+00:00"
    try:
        dt = datetime.fromisoformat(ts)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except ValueError:
        dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt


def parse_number(s: str) -> float:
    """è§£ææ•°å­—å­—ç¬¦ä¸²ï¼Œå¤„ç†ç©ºå€¼"""
    if not s or s.strip() == "":
        return 0.0
    try:
        return float(s.replace(",", ""))
    except ValueError:
        return 0.0


def analyze_csv(csv_path: Path, start_date: datetime, end_date: datetime, target_user: str = None) -> dict:
    """åˆ†æå•ä¸ª CSV æ–‡ä»¶"""
    stats = {
        "input_tokens_with_cache": 0,
        "input_tokens_without_cache": 0,
        "cache_read_tokens": 0,
        "output_tokens": 0,
        "total_tokens": 0,
        "requests": 0.0,
        "records": 0,
        "errored_records": 0,
        "by_model": defaultdict(lambda: {
            "input_tokens_with_cache": 0,
            "input_tokens_without_cache": 0,
            "cache_read_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "requests": 0.0,
            "records": 0,
        }),
        "by_user": defaultdict(lambda: {
            "input_tokens_with_cache": 0,
            "input_tokens_without_cache": 0,
            "cache_read_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "requests": 0.0,
            "records": 0,
        }),
        "by_day": defaultdict(lambda: {
            "input_tokens_with_cache": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "requests": 0.0,
            "records": 0,
        }),
        "active_days": set(),
        "users": set(),
    }

    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # è§£ææ—¶é—´æˆ³
            date_str = row.get("Date", "")
            if not date_str:
                continue

            try:
                timestamp = parse_timestamp(date_str)
            except (ValueError, TypeError):
                continue

            # æ£€æŸ¥æ—¶é—´èŒƒå›´
            if timestamp < start_date or timestamp > end_date:
                continue

            # æ£€æŸ¥ç”¨æˆ·è¿‡æ»¤
            user = row.get("User", "")
            if target_user and user != target_user:
                continue

            # æ£€æŸ¥è¯·æ±‚ç±»å‹
            kind = row.get("Kind", "")
            if "Errored" in kind or "No Charge" in kind:
                stats["errored_records"] += 1
                continue

            # è§£ææ•°æ®
            input_with_cache = parse_number(row.get("Input (w/ Cache Write)", "0"))
            input_without_cache = parse_number(row.get("Input (w/o Cache Write)", "0"))
            cache_read = parse_number(row.get("Cache Read", "0"))
            output_tokens = parse_number(row.get("Output Tokens", "0"))
            total_tokens = parse_number(row.get("Total Tokens", "0"))
            requests = parse_number(row.get("Requests", "0"))
            model = row.get("Model", "unknown")
            day = timestamp.date().isoformat()

            # ç´¯åŠ æ€»è®¡
            stats["input_tokens_with_cache"] += input_with_cache
            stats["input_tokens_without_cache"] += input_without_cache
            stats["cache_read_tokens"] += cache_read
            stats["output_tokens"] += output_tokens
            stats["total_tokens"] += total_tokens
            stats["requests"] += requests
            stats["records"] += 1
            stats["active_days"].add(day)
            stats["users"].add(user)

            # æŒ‰æ¨¡å‹ç»Ÿè®¡
            stats["by_model"][model]["input_tokens_with_cache"] += input_with_cache
            stats["by_model"][model]["input_tokens_without_cache"] += input_without_cache
            stats["by_model"][model]["cache_read_tokens"] += cache_read
            stats["by_model"][model]["output_tokens"] += output_tokens
            stats["by_model"][model]["total_tokens"] += total_tokens
            stats["by_model"][model]["requests"] += requests
            stats["by_model"][model]["records"] += 1

            # æŒ‰ç”¨æˆ·ç»Ÿè®¡
            stats["by_user"][user]["input_tokens_with_cache"] += input_with_cache
            stats["by_user"][user]["input_tokens_without_cache"] += input_without_cache
            stats["by_user"][user]["cache_read_tokens"] += cache_read
            stats["by_user"][user]["output_tokens"] += output_tokens
            stats["by_user"][user]["total_tokens"] += total_tokens
            stats["by_user"][user]["requests"] += requests
            stats["by_user"][user]["records"] += 1

            # æŒ‰å¤©ç»Ÿè®¡
            stats["by_day"][day]["input_tokens_with_cache"] += input_with_cache
            stats["by_day"][day]["output_tokens"] += output_tokens
            stats["by_day"][day]["total_tokens"] += total_tokens
            stats["by_day"][day]["requests"] += requests
            stats["by_day"][day]["records"] += 1

    return stats


def collect_stats(csv_files: list[Path], start_date: datetime, end_date: datetime,
                  target_user: str = None, username: str = None) -> dict:
    """æ”¶é›†æ‰€æœ‰ CSV æ–‡ä»¶çš„ç»Ÿè®¡æ•°æ®"""
    result = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "username": username or os.environ.get("USER", "unknown"),
            "machine": os.uname().nodename,
            "source": "cursor",
            "csv_files": [str(f) for f in csv_files],
        },
        "summary": {
            "input_tokens_with_cache": 0,
            "input_tokens_without_cache": 0,
            "cache_read_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "requests": 0.0,
            "records": 0,
            "errored_records": 0,
            "active_days": 0,
            "users_count": 0,
        },
        "by_model": {},
        "by_user": {},
        "by_day": {},
    }

    all_active_days = set()
    all_users = set()
    combined_by_model = defaultdict(lambda: {
        "input_tokens_with_cache": 0,
        "input_tokens_without_cache": 0,
        "cache_read_tokens": 0,
        "output_tokens": 0,
        "total_tokens": 0,
        "requests": 0.0,
        "records": 0,
    })
    combined_by_user = defaultdict(lambda: {
        "input_tokens_with_cache": 0,
        "input_tokens_without_cache": 0,
        "cache_read_tokens": 0,
        "output_tokens": 0,
        "total_tokens": 0,
        "requests": 0.0,
        "records": 0,
    })
    combined_by_day = defaultdict(lambda: {
        "input_tokens_with_cache": 0,
        "output_tokens": 0,
        "total_tokens": 0,
        "requests": 0.0,
        "records": 0,
    })

    for csv_file in csv_files:
        stats = analyze_csv(csv_file, start_date, end_date, target_user)

        # ç´¯åŠ æ€»è®¡
        result["summary"]["input_tokens_with_cache"] += stats["input_tokens_with_cache"]
        result["summary"]["input_tokens_without_cache"] += stats["input_tokens_without_cache"]
        result["summary"]["cache_read_tokens"] += stats["cache_read_tokens"]
        result["summary"]["output_tokens"] += stats["output_tokens"]
        result["summary"]["total_tokens"] += stats["total_tokens"]
        result["summary"]["requests"] += stats["requests"]
        result["summary"]["records"] += stats["records"]
        result["summary"]["errored_records"] += stats["errored_records"]

        all_active_days.update(stats["active_days"])
        all_users.update(stats["users"])

        # åˆå¹¶æŒ‰æ¨¡å‹ç»Ÿè®¡
        for model, model_stats in stats["by_model"].items():
            for key in model_stats:
                combined_by_model[model][key] += model_stats[key]

        # åˆå¹¶æŒ‰ç”¨æˆ·ç»Ÿè®¡
        for user, user_stats in stats["by_user"].items():
            for key in user_stats:
                combined_by_user[user][key] += user_stats[key]

        # åˆå¹¶æŒ‰å¤©ç»Ÿè®¡
        for day, day_stats in stats["by_day"].items():
            for key in day_stats:
                combined_by_day[day][key] += day_stats[key]

    result["summary"]["active_days"] = len(all_active_days)
    result["summary"]["users_count"] = len(all_users)
    result["by_model"] = dict(combined_by_model)
    result["by_user"] = dict(combined_by_user)
    result["by_day"] = dict(combined_by_day)

    return result


def format_tokens(n: float) -> str:
    """æ ¼å¼åŒ– token æ•°é‡"""
    n = int(n)
    if n >= 1_000_000:
        return f"{n/1_000_000:.2f}M"
    elif n >= 1_000:
        return f"{n/1_000:.1f}K"
    return str(n)


def print_summary(stats: dict):
    """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
    summary = stats["summary"]
    metadata = stats["metadata"]

    print("=" * 60)
    print("Cursor ä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 60)
    print(f"\nç”¨æˆ·: {metadata['username']} @ {metadata['machine']}")
    print(f"ç»Ÿè®¡å‘¨æœŸ: {metadata['start_date'][:10]} ~ {metadata['end_date'][:10]}")
    print(f"ç”Ÿæˆæ—¶é—´: {metadata['generated_at'][:19]}")
    print(f"æ•°æ®æº: {len(metadata['csv_files'])} ä¸ª CSV æ–‡ä»¶")

    print("\nğŸ“Š Token ä½¿ç”¨é‡")
    print("-" * 40)
    print(f"  è¾“å…¥ Token (å«ç¼“å­˜å†™):  {format_tokens(summary['input_tokens_with_cache']):>12}")
    print(f"  è¾“å…¥ Token (ä¸å«ç¼“å­˜):  {format_tokens(summary['input_tokens_without_cache']):>12}")
    print(f"  ç¼“å­˜è¯»å– Token:         {format_tokens(summary['cache_read_tokens']):>12}")
    print(f"  è¾“å‡º Token:             {format_tokens(summary['output_tokens']):>12}")
    print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"  æ€»è®¡ Token:             {format_tokens(summary['total_tokens']):>12}")

    print("\nğŸ“ˆ æ´»åŠ¨ç»Ÿè®¡")
    print("-" * 40)
    print(f"  API è¯·æ±‚æ•°:             {summary['requests']:>12.1f}")
    print(f"  è®°å½•æ•°:                 {summary['records']:>12}")
    print(f"  é”™è¯¯è®°å½•æ•°:             {summary['errored_records']:>12}")
    print(f"  æ´»è·ƒå¤©æ•°:               {summary['active_days']:>12}")
    print(f"  ç”¨æˆ·æ•°:                 {summary['users_count']:>12}")

    if stats["by_model"]:
        print("\nğŸ¤– æŒ‰æ¨¡å‹ç»Ÿè®¡")
        print("-" * 40)
        sorted_models = sorted(
            stats["by_model"].items(),
            key=lambda x: x[1]["total_tokens"],
            reverse=True
        )
        for model, model_stats in sorted_models[:5]:
            print(f"  {model}:")
            print(f"    è®°å½•: {model_stats['records']}, Token: {format_tokens(model_stats['total_tokens'])}")

    if stats["by_user"] and len(stats["by_user"]) > 1:
        print("\nğŸ‘¤ æŒ‰ç”¨æˆ·ç»Ÿè®¡")
        print("-" * 40)
        sorted_users = sorted(
            stats["by_user"].items(),
            key=lambda x: x[1]["total_tokens"],
            reverse=True
        )
        for user, user_stats in sorted_users[:5]:
            # ç®€åŒ–é‚®ç®±æ˜¾ç¤º
            display_user = user.split("@")[0] if "@" in user else user
            print(f"  {display_user}:")
            print(f"    è®°å½•: {user_stats['records']}, Token: {format_tokens(user_stats['total_tokens'])}")

    if stats["by_day"]:
        print("\nğŸ“… æŒ‰æ—¥æœŸç»Ÿè®¡")
        print("-" * 40)
        sorted_days = sorted(stats["by_day"].items())
        for day, day_stats in sorted_days:
            print(f"  {day}: {format_tokens(day_stats['total_tokens'])} ({day_stats['records']} æ¡è®°å½•)")

    print("\n" + "=" * 60)


def find_csv_files(directory: Path, pattern: str = "*.csv") -> list[Path]:
    """åœ¨ç›®å½•ä¸­æŸ¥æ‰¾ CSV æ–‡ä»¶"""
    return list(directory.glob(pattern))


def main():
    parser = argparse.ArgumentParser(
        description="ç»Ÿè®¡ Cursor ä½¿ç”¨æƒ…å†µï¼ˆä»å¯¼å‡ºçš„ CSV æ–‡ä»¶ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s usage.csv                    # åˆ†æå•ä¸ª CSV æ–‡ä»¶
  %(prog)s *.csv                        # åˆ†æå½“å‰ç›®å½•æ‰€æœ‰ CSV æ–‡ä»¶
  %(prog)s --dir ./data                 # åˆ†ææŒ‡å®šç›®å½•çš„ CSV æ–‡ä»¶
  %(prog)s --start 2026-01-24 --end 2026-01-30  # æŒ‡å®šæ—¥æœŸèŒƒå›´
  %(prog)s --user alice@example.com     # åªç»Ÿè®¡ç‰¹å®šç”¨æˆ·
  %(prog)s --output stats.json          # è¾“å‡ºåˆ° JSON æ–‡ä»¶
        """
    )
    parser.add_argument("csv_files", nargs="*", help="CSV æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰")
    parser.add_argument("--dir", type=str, help="CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•")
    parser.add_argument("--start", type=str, help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end", type=str, help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--days", type=int, help="ç»Ÿè®¡æœ€è¿‘ N å¤©")
    parser.add_argument("--user", type=str, help="åªç»Ÿè®¡ç‰¹å®šç”¨æˆ·")
    parser.add_argument("--username", type=str, help="ç”¨æˆ·åï¼ˆç”¨äºå›¢é˜Ÿæ±‡æ€»ï¼‰")
    parser.add_argument("--output", "-o", type=str, help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--json", action="store_true", help="ä»…è¾“å‡º JSONï¼ˆä¸æ‰“å°æ‘˜è¦ï¼‰")

    args = parser.parse_args()

    # æ”¶é›† CSV æ–‡ä»¶
    csv_files = []
    if args.csv_files:
        for pattern in args.csv_files:
            # Check if it's an actual file path first, then try glob
            p = Path(pattern)
            if p.is_file():
                csv_files.append(p)
            else:
                csv_files.extend(Path(".").glob(pattern))
    elif args.dir:
        csv_files = find_csv_files(Path(args.dir))
    else:
        # é»˜è®¤æŸ¥æ‰¾å½“å‰ç›®å½•çš„ CSV æ–‡ä»¶
        csv_files = find_csv_files(Path("."))

    if not csv_files:
        print("é”™è¯¯: æœªæ‰¾åˆ° CSV æ–‡ä»¶", file=sys.stderr)
        print("ç”¨æ³•: cursor_stats.py [csvæ–‡ä»¶...] æˆ– cursor_stats.py --dir <ç›®å½•>", file=sys.stderr)
        sys.exit(1)

    csv_files = [f for f in csv_files if f.is_file()]
    if not csv_files:
        print("é”™è¯¯: æœªæ‰¾åˆ°æœ‰æ•ˆçš„ CSV æ–‡ä»¶", file=sys.stderr)
        sys.exit(1)

    # ç¡®å®šæ—¥æœŸèŒƒå›´
    now = datetime.now(timezone.utc)
    today = now.replace(hour=23, minute=59, second=59, microsecond=999999)

    if args.start and args.end:
        start_date = datetime.strptime(args.start, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        end_date = datetime.strptime(args.end, "%Y-%m-%d").replace(
            hour=23, minute=59, second=59, microsecond=999999, tzinfo=timezone.utc
        )
    elif args.days:
        end_date = today
        start_date = (now - timedelta(days=args.days - 1)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )
    else:
        # é»˜è®¤ï¼šæœ¬å‘¨ï¼ˆå‘¨ä¸€åˆ°ä»Šå¤©ï¼‰
        end_date = today
        days_since_monday = now.weekday()
        start_date = (now - timedelta(days=days_since_monday)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )

    # æ”¶é›†ç»Ÿè®¡
    stats = collect_stats(csv_files, start_date, end_date, args.user, args.username)

    # è¾“å‡º
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        if not args.json:
            print(f"ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {args.output}")

    if args.json:
        print(json.dumps(stats, ensure_ascii=False, indent=2))
    elif not args.output:
        print_summary(stats)
    else:
        print_summary(stats)


if __name__ == "__main__":
    main()
